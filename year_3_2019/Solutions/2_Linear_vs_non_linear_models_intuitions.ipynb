{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.201406Z",
     "start_time": "2019-03-14T13:37:03.932823Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "\n",
    "import simulate\n",
    "from simulate import simulate_data, bayes_boundary, parabola, circle, boundary_1\n",
    "import plotting\n",
    "from plotting import plot_two_variables, plot_roc_curve, plot_clf_decision_boundary, plot_strategy_curve, plot_classifier_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "\n",
    "For the purpose of teaching, we will simulate a dataset where we know how the targets are assigned.<br>\n",
    "\n",
    "The dataset will contain 3000 rows and two columns, so that we can visually understand what is going on. <br>\n",
    "\n",
    "<b> Note: this is a very ideal simple scenario for visual understanding. This will not work in real life\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.205549Z",
     "start_time": "2019-03-14T13:37:03.933Z"
    }
   },
   "outputs": [],
   "source": [
    "N_population = 3000\n",
    "data = simulate_data(population_size=N_population, N_cont_variables=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Bayesian Classifier boundary functions\n",
    "\n",
    "Here we define the targets on our simulated dataset with a non linear separation. <br>\n",
    "We will plot the points later for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.206427Z",
     "start_time": "2019-03-14T13:37:03.935Z"
    }
   },
   "outputs": [],
   "source": [
    "probs,targets,coordinates = bayes_boundary(\n",
    "    boundary_func=parabola,\n",
    "    print_median=False, \n",
    "    x = data['col_0'], y = data['col_1'],\n",
    "    max_accuracy=1)\n",
    "\n",
    "# assign it to the dataset\n",
    "data['y']=targets\n",
    "\n",
    "#show the first five values\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Bayesian Classifier boundaries\n",
    "\n",
    "Let's plot the points for visualising the data we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.207482Z",
     "start_time": "2019-03-14T13:37:03.936Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_two_variables(df=data, fig=fig, ax=ax,plot_separation_boundary=True, boundary_func=parabola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "\n",
    "Let's try to build some models to run the classifications.<br>\n",
    "\n",
    "We will try the following models:\n",
    "- **Logistic regression**: it is a linear model, which is the most common in building score cards. We will try to illustrate the limitations of this model\n",
    "- **Random forest**: a tree-based ensemble model, which has more flexibility in splitting classes, but is prone to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "We need to separate our sample in a training sample (where we build the model) and a test sample (where we can evaluate the model performance).<br>\n",
    "The `train_test_split` function of scikit-learn is helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.209154Z",
     "start_time": "2019-03-14T13:37:03.939Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.210443Z",
     "start_time": "2019-03-14T13:37:03.940Z"
    }
   },
   "outputs": [],
   "source": [
    "# it is always a good idea to explicitly specify which columns represent the data to train on, and which columns are\n",
    "# the targets\n",
    "X = data[['col_0','col_1']]\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.211416Z",
     "start_time": "2019-03-14T13:37:03.941Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.375, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.212377Z",
     "start_time": "2019-03-14T13:37:03.943Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model - logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.213424Z",
     "start_time": "2019-03-14T13:37:03.944Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.214490Z",
     "start_time": "2019-03-14T13:37:03.945Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.215405Z",
     "start_time": "2019-03-14T13:37:03.946Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_roc_curve(lr,X_train,y_train, ax=ax, color='darkgreen',label = 'Train AUC')\n",
    "plot_roc_curve(lr,X_test,y_test, ax=ax, color='salmon', label = 'Test AUC ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The AUC seems quite good. But let's have a look at the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.216415Z",
     "start_time": "2019-03-14T13:37:03.947Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "from plotting import plot_clf_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.217339Z",
     "start_time": "2019-03-14T13:37:03.948Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_clf_decision_boundary(df=data,clf =lr, plot_classifier_boundary = True, plot_classifier_prob_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a non linear model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.218284Z",
     "start_time": "2019-03-14T13:37:03.950Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.219235Z",
     "start_time": "2019-03-14T13:37:03.951Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.220150Z",
     "start_time": "2019-03-14T13:37:03.952Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.221125Z",
     "start_time": "2019-03-14T13:37:03.953Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_roc_curve(rf,X_train,y_train, ax=ax, color='darkgreen',label = 'Train AUC')\n",
    "plot_roc_curve(rf,X_test,y_test, ax=ax, color='purple', label = 'Test AUC ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.222016Z",
     "start_time": "2019-03-14T13:37:03.954Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_clf_decision_boundary(df=X_train, target = y_train,ax=ax, clf =rf, \n",
    "                           plot_classifier_boundary = True,plot_classifier_prob_map=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Random Forest separates the classes (almost) perfectly. But, this is too ideal to be true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A more realistic scenario\n",
    "In this simulated example, we have a clear (tough non-linear) separation between the two classes.<br>\n",
    "Let's generate a more realistic scenario, where there is no perfect separation possible, i.e the irreducible error is not zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.222937Z",
     "start_time": "2019-03-14T13:37:03.955Z"
    }
   },
   "outputs": [],
   "source": [
    "probs,targets,coordinates = bayes_boundary(\n",
    "    boundary_func=parabola,\n",
    "    print_median=False, \n",
    "    x = data['col_0'], y = data['col_1'],\n",
    "    max_accuracy=0.92) # in this case, 8% of my observations will randomly swap classes\n",
    "\n",
    "# assign it to the dataset\n",
    "data['y']=targets\n",
    "\n",
    "#show the first five values\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.223843Z",
     "start_time": "2019-03-14T13:37:03.956Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(plotting)\n",
    "from plotting import plot_two_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.224939Z",
     "start_time": "2019-03-14T13:37:03.957Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_two_variables(df=data, fig=fig, ax=ax,plot_bayes_colourmap = False, plot_separation_boundary=True,boundary_func=parabola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, now some red points are deep in the blue area and viceversa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's redo now the train test split, as we have a new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.226055Z",
     "start_time": "2019-03-14T13:37:03.958Z"
    }
   },
   "outputs": [],
   "source": [
    "# it is always a good idea to explicitly specify which columns represent the data to train on, and which columns are\n",
    "# the targets\n",
    "X = data[['col_0','col_1']]\n",
    "y = data['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.375, random_state=42)\n",
    "\n",
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train again the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.227087Z",
     "start_time": "2019-03-14T13:37:03.960Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.228176Z",
     "start_time": "2019-03-14T13:37:03.961Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_roc_curve(lr,X_train,y_train, ax=ax, color='darkgreen',label = 'Train AUC')\n",
    "plot_roc_curve(lr,X_test,y_test, ax=ax, color='purple', label = 'Test AUC ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.229151Z",
     "start_time": "2019-03-14T13:37:03.962Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_clf_decision_boundary(df=data,clf =lr, plot_classifier_boundary = True,plot_classifier_decisions=False,\n",
    "                           plot_classifier_prob_map=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now, the Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.230112Z",
     "start_time": "2019-03-14T13:37:03.964Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.231437Z",
     "start_time": "2019-03-14T13:37:03.964Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_roc_curve(rf,X_train,y_train, ax=ax, color='darkgreen',label = 'Train AUC')\n",
    "plot_roc_curve(rf,X_test,y_test, ax=ax, color='purple', label = 'Test AUC ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happend? \n",
    "What can you tell from the AUC score on the training set and the AUC score on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.232435Z",
     "start_time": "2019-03-14T13:37:03.966Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(15, 20))\n",
    "ax[0].set_title('Training set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_train, target = y_train,fig = fig,ax = ax[0], clf =rf, \n",
    "                           plot_classifier_boundary = False,plot_classifier_prob_map=True)\n",
    "ax[1].set_title('Test set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_test, target = y_test,fig = fig, ax = ax[1], clf =rf, \n",
    "                           plot_classifier_boundary = False,plot_classifier_prob_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE\n",
    "\n",
    "- check the hyperparameters of the RandomForest (try `help(RandomForestClassifier)`)\n",
    "- try modifing the hypterparameteres until you reach a better generalization and you reduce the overfitting\n",
    "\n",
    "**tip** consider the `n_estimators` and the `max_depth`\n",
    "\n",
    "\n",
    "**Note** the training time increases with n_estimators:\n",
    "* with 10 trees takes cca 3-4 seconds\n",
    "* with 100 trees takes cca 20-30 seconds\n",
    "* with 1000 trees takes cca 8-10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.236075Z",
     "start_time": "2019-03-14T13:37:03.969Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_exercise = RandomForestClassifier(\n",
    "n_estimators=<enter_value>\n",
    "max_depth=<enter_value>\n",
    ")\n",
    "rf_exercise.fit(X_train,y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "plot_roc_curve(rf_exercise,X_train,y_train, ax=ax, color='darkgreen',label = 'Train AUC')\n",
    "plot_roc_curve(rf_exercise,X_test,y_test, ax=ax, color='purple', label = 'Test AUC ')\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(15, 20))\n",
    "ax[0].set_title('Training set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_train, target = y_train,fig = fig,ax = ax[0], clf =rf_exercise, \n",
    "                           plot_classifier_boundary = True,plot_classifier_prob_map=True)\n",
    "ax[1].set_title('Test set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_test, target = y_test,fig = fig, ax = ax[1], clf =rf_exercise, \n",
    "                           plot_classifier_boundary = True,plot_classifier_prob_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get an idea of how hyperparemeters affect the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.237490Z",
     "start_time": "2019-03-14T13:37:03.970Z"
    }
   },
   "outputs": [],
   "source": [
    "from plotting import plot_score_vs_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.239216Z",
     "start_time": "2019-03-14T13:37:03.971Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_test = RandomForestClassifier(random_state=42,n_estimators=10, max_depth=3)\n",
    "param_grid={'n_estimators': range(1, 60, 1)}\n",
    "\n",
    "plot_score_vs_hyperparameter(rf_test,X_train,y_train, scoring_metric = 'roc_auc',param_grid=param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.241029Z",
     "start_time": "2019-03-14T13:37:03.972Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_test = RandomForestClassifier(random_state=42,n_estimators=10)\n",
    "param_grid={'min_samples_split': range(2, 800, 20)}\n",
    "\n",
    "plot_score_vs_hyperparameter(rf_test,X_train,y_train, scoring_metric = 'roc_auc',param_grid=param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.242801Z",
     "start_time": "2019-03-14T13:37:03.973Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid={'max_depth': list(range(1, 20, 1))}\n",
    "#param_grid={'min_samples_split': range(2, 800, 20)}\n",
    "plot_score_vs_hyperparameter(RandomForestClassifier(n_estimators=40,max_depth=8,min_samples_split=500),\n",
    "                             X_train,y_train, \n",
    "                             scoring_metric = 'roc_auc',\n",
    "                             param_grid=param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.244229Z",
     "start_time": "2019-03-14T13:37:03.974Z"
    }
   },
   "outputs": [],
   "source": [
    "final_rf = RandomForestClassifier(n_estimators=40,max_depth=8,min_samples_split=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.246108Z",
     "start_time": "2019-03-14T13:37:03.975Z"
    }
   },
   "outputs": [],
   "source": [
    "final_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T13:37:16.247456Z",
     "start_time": "2019-03-14T13:37:03.976Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(15, 20))\n",
    "ax[0].set_title('Training set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_train, target = y_train,fig = fig,ax = ax[0], clf =final_rf, \n",
    "                           plot_classifier_boundary = True,plot_classifier_prob_map=True)\n",
    "ax[1].set_title('Test set', size='xx-large')\n",
    "plot_clf_decision_boundary(df=X_test, target = y_test,fig = fig, ax = ax[1], clf =final_rf, \n",
    "                           plot_classifier_boundary = True,plot_classifier_prob_map=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
