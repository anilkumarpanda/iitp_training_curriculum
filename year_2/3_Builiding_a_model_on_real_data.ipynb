{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling \n",
    "\n",
    "* In this notebook we will build a machine learning model using the data we processed in the previous session.<br>\n",
    "* Scikit-learn is a very popular library for Machine Learning in Python. We will explore its API and use it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "#for dealing with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for changing working directory\n",
    "import os\n",
    "#for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "#plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scikit learn packages\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, \\\n",
    "                            precision_score, recall_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/processed_data.csv',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check again the columns we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some questions we should ask ourselves before we start modelling\n",
    "### What is the goal of our model?\n",
    "* We want to predict the target y (default yes/no) based on a set of features/risk drivers X\n",
    "* We already did some preprocessing of the data in the previous notebook, however, we not all the variables that we have make sense\n",
    "* For instance, `ID` cannot have any predictive power, so we should drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop('ID',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have enough defaults in our portfolio?\n",
    "Let's check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['default.payment.next.month'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6636 defaults (22%), and 23364 (78%) of no defaults.<br>\n",
    "The targets are not balances, but this is not a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much data do we use for training our model, and how much for validating it?\n",
    "The general rule of thumbs for splitting the data is 70%/30% or 80%/20%.<br>\n",
    "What matters is that we keep having enough targets in our sample, and that the distributions between our training and test set do not change. <br>\n",
    "We can use the train_test_split() function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let' split it in 75% training and 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(['default.payment.next.month','MARRIAGE_OTHER','SEX_MALE','EDUCATION_UNKNOWN'], axis=1),\n",
    "    data['default.payment.next.month'],test_size=0.25)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data[['PAY_1']],\n",
    "#                                                     data['default.payment.next.month'],test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sanity check \n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should check how the targets are distributed in the train and test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "Check the target distribution in the two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "*** your solution here ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one should expect:\n",
    "* it follows the train test split (i.e. cca 75% of the bads are in the training set and 25% of the bads are in the test set)\n",
    "* the distribution within the sample is cca 22% of bads, as in the complete sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the features?\n",
    "We do not want that the features are very different in the train and test set, because our validation might be wrong if this is the case.<br>\n",
    "There are different approaches to estimate the stability of the features distribution, but for the sake of simplicity we are happy with checking if the mean of the distribution is similar or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in X_train.columns:\n",
    "    print('\\ncolumn name:',col)\n",
    "    print('training mean %.2f'%(X_train[col].mean()),' - test mean %.2f'%(X_test[col].mean()),\n",
    "          ' - train/test mean: %.2f'%(X_train[col].mean()/X_test[col].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first model\n",
    "Let's start simple and classical, and begin with a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "In the scikit learn API, training and algorithm is done bu the `fit()` function.\n",
    "One can use the `predict()` or `predict_proba()` to predict the outcome or predict the probability for the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a logisitc regression is very fast, it took less than a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it returns an array of 0 and 1\n",
    "predictions = lr.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_probs=lr.predict_proba(X_test)\n",
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, predicted_proba returns a matrix of 7500 rows and two columns. Let's read the API documentation to understand why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(lr.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says that it returns the probability of the classes.<br>\n",
    "It means that the first array will show the probability of class 0 (no defaults) and the second array shows the probability of class 1 (defaults).<br>\n",
    "We are interested in the probability of defaults, hence the probability of being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_probs[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well does my model perform?\n",
    "We need some metric to evaluate the model.<br>\n",
    "`sklearn.metrics` offers a lot of them.<br>\n",
    "Let's see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy: %.3f'%accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr,X_train,y_train,threshold=0.5,cmap=plt.cm.Oranges, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's wrong here?\n",
    "* The model has a decent precision (78%) but the confusion matrix looks a bit weird, right? What is going on here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson learned: some metrics might be misleading - we need to check more than just the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Precision:',precision_score(y_test,predictions))\n",
    "print('Recall:',recall_score(y_test,predictions))\n",
    "print('AUC score: %.3f'%(roc_auc_score(y_test,predicted_probs[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And if I change my model threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "predictions_015 = np.array(list(map(lambda x: 1 if x else 0, lr.predict_proba(X_test)[:,1]>threshold)))\n",
    "print('Accuracy: %.3f'%accuracy_score(y_test,predictions_015))\n",
    "print('Precision:',precision_score(y_test,predictions_015))\n",
    "print('Recall:',recall_score(y_test,predictions_015))\n",
    "print('AUC score: %.3f'%(roc_auc_score(y_test,predicted_probs[:,1])))\n",
    "\n",
    "plot_confusion_matrix(lr,X_train,y_train,threshold=threshold,cmap=plt.cm.Oranges, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC is threshold indipendent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotting import plot_precision_recall_curve,plot_roc_curve\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_roc_curve(lr,X_train,y_train, ax, label='train')\n",
    "plot_roc_curve(lr,X_test,y_test, ax, label='test',color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a non-linear model\n",
    "\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: \n",
    "create and train a Random forest classifier. <br>\n",
    "tip: check how we created and trained a Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "*** your solution here ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_roc_curve(rf_default,X_train,y_train, ax, label='train')\n",
    "plot_roc_curve(rf_default,X_test,y_test, ax, label='test',color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another curve we would like to see is the precision vs recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotting import plot_precision_recall_curve\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_precision_recall_curve(rf_default,X_train,y_train, ax, label='train')\n",
    "plot_precision_recall_curve(rf_default,X_test,y_test, ax, label='test',color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "what can you conclude from the ROC AUC curve and the precision vs recall curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** answer ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Let' try to evaluate our models in terms of impact on the portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotting import plot_strategy_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_strategy_curve(rf_default,X_train,y_train, ax, label=' RF train',color='darkorange')\n",
    "plot_strategy_curve(rf_default,X_test,y_test, ax, label='RFtest',linestyle='--', color='darkorange')\n",
    "plot_strategy_curve(lr,X_train,y_train, ax, label='LR train',color = 'darkblue')\n",
    "plot_strategy_curve(lr,X_test,y_test, ax, label='LR test',color = 'darkblue',linestyle='--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profit\n",
    "As before, let's assume that for every good client we make 1200 euros, and for every bad client we loose 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_of_bads = 10000\n",
    "profit_of_goods = 1200\n",
    "\n",
    "from plotting import plot_classifier_profit\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_classifier_profit(rf_default,X_test,y_test, ax, cost_of_bads, profit_of_goods, label='RFtest',linestyle='--', color='darkorange')\n",
    "plot_classifier_profit(lr,X_test,y_test, ax,cost_of_bads, profit_of_goods, label='LR test',color = 'darkblue',linestyle='--')\n",
    "\n",
    "ax.set_ylim(-1e6,6e5)\n",
    "ax.set_xlim(-0,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE\n",
    "\n",
    "1. Test different Random Forest hyperparameters.\n",
    "2. What is the effect of each change of the parametrs to model performance & ofer/under-fitting?\n",
    "\n",
    "Note: Things you can try:\n",
    "1. more trees\n",
    "2. set max_depth\n",
    "3. set min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, \n",
    "#                             criterion='gini',\n",
    "                            min_samples_leaf=50,\n",
    "#                             max_depth=6,\n",
    "                            random_state=24423,\n",
    "                            n_jobs=-1,\n",
    "                           )\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_roc_curve(rf,X_train,y_train, ax, label='train')\n",
    "plot_roc_curve(rf,X_test,y_test, ax, label='test',color = 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the model depend on the hyperparameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotting import plot_score_vs_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_test = RandomForestClassifier(max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=0.0001, min_samples_split=0.5,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=1)\n",
    "param_grid={'n_estimators': range(1, 111, 10)}\n",
    "\n",
    "plot_score_vs_hyperparameter(rf_test,X_train,y_train, scoring_metric = 'roc_auc',param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_test = RandomForestClassifier(n_estimators=70)\n",
    "param_grid={'max_depth': range(1, 81, 5)}\n",
    "\n",
    "plot_score_vs_hyperparameter(rf_test,X_train,y_train, scoring_metric = 'roc_auc',param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_cv = RandomForestClassifier(max_features=None)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 150, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_features += [x for x in range(3,30,1)]\n",
    "# Maximmax_featuresum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 30, num = 30)]\n",
    "max_depth.append(None)\n",
    "#max_dept=None\n",
    "min_impurity_decrease = np.linspace(0.0001, 0.3, num = 30)\n",
    "# Minimum number of samples required to split a node\n",
    "#min_samples_split = [int(x) for x in np.linspace(2, 10, num = 9)]\n",
    "#min_samples_split = [x for x in np.linspace(0.0001, 1, num = 30)]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [x for x in np.linspace(0.0001, 0.5, num = 300)]\n",
    "# Method of selecting samples for training each t\n",
    "class_weight = [None, 'balanced']\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'min_impurity_decrease':min_impurity_decrease,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#             'max_depth': max_depth, 'max_features': max_features,\n",
    "#                'min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rf_random = RandomizedSearchCV(estimator = rf_cv, scoring='roc_auc',\n",
    "                               param_distributions = random_grid, n_iter = 400, \n",
    "                               cv = 3, verbose=1, random_state=42, n_jobs = -1,\n",
    "                               return_train_score=True)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see the top 5 scores that the grid search has found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(rf_random.cv_results_)\n",
    "cv_res[['mean_train_score','mean_test_score','params']].sort_values(by='mean_test_score',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_ests = cv_res.sort_values(by='mean_test_score',ascending=False).head(3).T\n",
    "best_ests.columns=['1st','2nd','3rd']\n",
    "best_ests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_best.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_no_overfit= joblib.load('non_overfit_rf.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# rf2 = RandomForestClassifier(max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=0.0001, min_samples_split=0.4828103448275862,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=673, n_jobs=1)\n",
    "rf2 = RandomForestClassifier(max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=0.0001, min_samples_split=0.5,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=1)\n",
    "rf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# joblib.dump(rf2,'non_overfit_rf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the best random forest compare to our logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(plotting)\n",
    "from plotting import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_roc_curve(rf_best_no_overfit,X_train,y_train, ax, label='RF train',color='green',linestyle='--')\n",
    "plot_roc_curve(rf_best_no_overfit,X_test,y_test, ax, label='RF test',color = 'green')\n",
    "plot_roc_curve(lr,X_train,y_train, ax, label='LR train',color='darkred',linestyle='--')\n",
    "plot_roc_curve(lr,X_test,y_test, ax, label='LR test',color = 'darkred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_precision_recall_curve(rf_best_no_overfit,X_train,y_train, ax, label='RF train',color='green',linestyle='--')\n",
    "plot_precision_recall_curve(rf_best_no_overfit,X_test,y_test, ax, label='RF test',color = 'green')\n",
    "plot_precision_recall_curve(lr,X_train,y_train, ax, label='LR train',color='darkred',linestyle='--')\n",
    "plot_precision_recall_curve(lr,X_test,y_test, ax, label='LR test',color = 'darkred')\n",
    "ax.set_xlim(0.0001,1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr2,X_test,y_test,classes=['Good','Bad'],ax=ax[0],threshold=0.1,cmap=plt.cm.Oranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf_best_no_overfit,X_test,y_test,classes=['Good','Bad'],threshold=0.18,cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot_strategy_curve(rf_best_no_overfit,X_train,y_train, ax, label='RF train',color='green',linestyle='--')\n",
    "plot_strategy_curve(rf_best_no_overfit,X_test,y_test, ax, label='RF test',color = 'green')\n",
    "plot_strategy_curve(lr,X_train,y_train, ax, label='LR train',color='darkred',linestyle='--')\n",
    "plot_strategy_curve(lr,X_test,y_test, ax, label='LR test',color = 'darkred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_of_bads = 10000\n",
    "profit_of_goods = 1200\n",
    "# cost_of_bads = 1\n",
    "# profit_of_goods = 1\n",
    "\n",
    "from plotting import plot_classifier_profit\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "#plot_classifier_profit(rf2,X_train,y_train, ax, cost_of_bads, profit_of_goods, label=' RF train',color='darkorange')\n",
    "plot_classifier_profit(rf_best_no_overfit,X_test,y_test, ax, cost_of_bads, profit_of_goods, label='RFtest',linestyle='--', color='darkorange')\n",
    "#plot_classifier_profit(lr,X_train,y_train, ax, cost_of_bads, profit_of_goods, label='LR train',color = 'darkblue')\n",
    "plot_classifier_profit(lr,X_test,y_test, ax,cost_of_bads, profit_of_goods, label='LR test',color = 'darkblue',linestyle='--')\n",
    "\n",
    "ax.set_ylim(-1e6,1.2e6)\n",
    "ax.set_xlim(0,0.45)\n",
    "\n",
    "# ax.set_ylim(0.0001,3e7)\n",
    "# ax.semilogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And what about the Model interpretability?\n",
    "* Can we say something about which features are important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "pd.Series(rf_best_no_overfit.feature_importances_,index = X_train.columns).sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is time, a follow up exercise:\n",
    "* Try to simplify your model by keeping a lower number of features\n",
    "* Try improving your model by creating more features (and later select the best ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:IITP]",
   "language": "python",
   "name": "conda-env-IITP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
